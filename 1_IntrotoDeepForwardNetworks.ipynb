{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student: Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 530    \n",
    "\n",
    "## Introduction to Deep Neural Networks\n",
    "\n",
    "### Steve Elston\n",
    "\n",
    "### 1.0 Overview\n",
    "\n",
    "This lesson introduces you to the basics of neural network architecture in the form of deep forward networks. This architecture is the quintessential deep neural net architecture. In this lesson you will master the following:\n",
    "\n",
    "- Why is deep learning important and how it relates to representation, learning and inference.\n",
    "- How a basic Preceptron works.\n",
    "- How to apply different types of loss functions. \n",
    "- Understand why nonlinear activation is important and why rectified linear units are a good choice.\n",
    "- How back propagation works, and how you apply the chain rule of calculus to determine gradient. \n",
    "- Understand the architectural trade-off between depth and width in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Why is deep learning important?\n",
    "\n",
    "Deep learning methods are a form of **artificial intelligence (AI)** or **machine intelligence**. More specifically, deep learning algorithms are a type of **machine learning**. \n",
    "\n",
    "What properties does machine intelligence require? There have been many answers to this question over the history of computing. In this case, we will take a practical view, sometimes known as **weak AI**. There are three key properties an intelligent machine must have. Deep learning algorithms are one of a few classes of algorithms that can do the following, essential to machine intelligence:\n",
    "\n",
    "1. **Representation:** An intelligent machine must be able to represent a model of the world it interacts with in a general manner. Representation is key to intelligence. Without a good representation the best learning and inference algorithms will struggle. Whereas, good representation can greatly facilitate learning and inference. In conventional machine learning the representation is model and a set of features. The representation is limited to what the features can provide directly. Deep learning algorithms, on the other hand, learn learn complex representations from raw features. This behavior allows deep learning algorithms to approximate complex relationships. Further, the representations learned often generalize well, up to a point. \n",
    "2. **Learning:** As you likely guessed from the very name, deep learning algorithms learn from data. Whereas, conventional machine learning is focused on inference,deep learning algorithms learn both inference and representations. As a result, deep leaning algorithms are more complex and therefore harder to train than conventional machine learning algorithms.  \n",
    "3. **Inference:** Any machine intelligence algorithm must be able to perform inference. The inference is the result produced given new input data. To be useful, the inferences produced by a machine intelligence algorithm must **generalize** beyond the cases used for learning or training. Good generalization requires both good representations and learning which can deal with the complexity of diverse situations. Some deep learning algorithms can approach human levels of performance in inference tasks such as recognizing objects in images or understanding natural speech. \n",
    "\n",
    "The figure below shows a highly abstracted view of machine intelligence, showing the relationship between representation, learning and inference. In simple terms, the representation is learned and then used to make inferences. Errors in the inferences can be used to improve the learning of the representation.   \n",
    "\n",
    "<img src=\"Figures/MachineIntelligence.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "\n",
    "<center>Schematic for creating machine intelligence</center>\n",
    "\n",
    "**That's it!** The entire rest of this course will focus on just these three points: representation, learning and inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installing Keras\n",
    "\n",
    "This notebook will provides a first look at using the Keras package to define, train and evaluate deep learning models with Keras. The Keras package is a wrapper on TensorFlow, intended to abstract and simplify the definition, training and execution of TensorFlow deep learning models. You can find extensive well-written documentation for Keras [here](https://keras.io/). The book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition) by Fran√ßois Chollet, the creator of Keras, provides in-depth examples and discussion on a wide range of deep learning applications.      \n",
    "\n",
    "By the end of this lesson you will be able to work with basic feedforward architecture multi-layer neural nets. Feedforward networks are one of a class of basic models called **sequential models** which are easy to define with Keras. Some basic regularization is introduced. Additional regularization methods are covered in a subsequent lesson. \n",
    "\n",
    "Keras is part of the base package, as of the release of TensorFlow 2. Before proceeding make sure you have TensorFlow 2 installed in your environment. [Follow these instructions.](https://www.tensorflow.org/install).    \n",
    "\n",
    "****\n",
    "**Note:** As an alternative to working with a local installation, you may choose to use the [Google Colabratory](https://colab.research.google.com/?utm_source=scs-index). The Colabrotory virtual environment includes Anaconda, TensorFlow and Keras. However, the use of shared resources can result in slow execution.    \n",
    "****\n",
    "\n",
    "****\n",
    "**Note:** This notebook was constructed and tested using Anaconda 3 with Python 3. It is assumed that the standard Anaconda stack has been installed.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow \n",
    "# !pip install keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import keras.utils.np_utils as ku\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Forward propagation: The representation problem\n",
    "\n",
    "To create useful neutral network we need a **representation** that has two important properties.   \n",
    "\n",
    "First, there needs to be a way to represent complex functions of the input. Without this property, nothing is gained, since there are numerous machine learning algorithms that work with simple representations. We will spend the rest of this section exploring this problem.   \n",
    "\n",
    "Second, the representation needs to be learnable. Quite obviously, no machine intelligence representation is useful if there is not a practical algorithm to learn it. We will take up this problem in another section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear networks\n",
    "\n",
    "Let's start with the simplest possible network. It has inputs, and an output. The output is a **afine transformation** of the input values. We say this network performs an afine transformation since there is a bias term $b$. \n",
    "\n",
    "\n",
    "<img src=\"Figures/LinearNetwork.jpg\" alt=\"Drawing\" style=\"width:400px; height:250px\"/>\n",
    "\n",
    "<center>**Figure 2.1**\n",
    "**A simple afine network**</center>\n",
    "\n",
    "This output $y$ of this network is just:\n",
    "\n",
    "$$y = f(x) = \\sum_i w_i \\cdot x_i + b$$\n",
    "\n",
    "This network performs linear regression. Being able to perform only afine transformations, it can't do anything else. \n",
    "\n",
    "This representation is certainly learnable. However, it does not gain us anything over familiar linear regression methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The preceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's have a look at a simple **preceptron** model. The perceptron was proposed by Rosenblatt (1962). He built on the earlier attempts at a neural network models by McCulloch and Pitts (1943) and Heeb (1949). The perceptron adds **nonlinear activation** to the afine network. \n",
    "\n",
    "\n",
    "<img src=\"Figures/Preceptron.jpg\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Figure 2.2 Schematic of perceptron with nonlinear activation</center>\n",
    "\n",
    "The output $y$ of the perceptron is given by the following:\n",
    "\n",
    "$$y = f(x) = \\sigma \\Big( \\sum_i w_i \\cdot x_i + b \\Big)$$\n",
    "\n",
    "The output of the network is now nonlinear, give the **activation function** $\\sigma(x)$. \n",
    "\n",
    "But, the preceptron is nothing more than a logistic regression classifier. The fact that the preceptron could only solve linearly separable problems was pointed out by Minsky and Papert (1969). The failure of the preceptron to learn an **exclusive or (XOR)** function is well known. See for example, Section 6.1 in GBC. \n",
    "\n",
    "Again, this representation is certainly learnable. However, as before, it does not gain us anything over well known logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Forward networks - We're gonna need a better representation!\n",
    "\n",
    "The problem with the  perceptron is one of representations. There is no way that this simple network can represent anything but a linearly separable function. To represent more complex functions, we need a more complex network. In more technical terms we need a network with greater **model capacity**. \n",
    "\n",
    "What we need is a network with layers of **hidden nodes**. The figure below shows a simple example of a neural network with one **hidden layer** with two nodes. Since every node (including inputs) is connected to every other node we call this architecture a **fully connected neural network**.\n",
    "\n",
    "\n",
    "<img src=\"Figures/Hidden.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center>**Figure 2.3  \n",
    "Fully connected neural network with single hidden layer**</center>\n",
    "\n",
    "Let's walk through some aspects of these diagrams. \n",
    "\n",
    "1. The neural network is divided into three layers. The input layer, the hidden layer and the output layer. \n",
    "2. The values in the input layer are multiplied by a weight matrix, $W^1$.\n",
    "3. The nodes in the hidden layer sum their inputs and add a bias term, $b^1$. \n",
    "4. The outputs of the hidden layer nodes are multiplied by a weight vector, $W^2$.\n",
    "5. The output layer sums the inputs and adds another bias term, $b^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural network architectures - Finding representations\n",
    "\n",
    "The representations achievable by neural network with just a single hidden layer  are quite powerful. In fact, Cybenko (1989) showed that such a network with an infinite number of hidden units using sigmoidal activation can approximate any arbitrary function. Hornik (1991) generalized this to apply to any activation function. We call this theorem the **universal approximation theorem**.  \n",
    "\n",
    "A universal approximation theorem may see like a really exciting development; especially if you are a machine intelligence nerd. However, one must be circumspect when viewing such a result. A representation with an infinite number of nodes cannot be learned in any practical sense. Still it is comforting to know that, at least in principle, a representation can be learned for arbitrarily complex problems. \n",
    "\n",
    "While infinitely wide networks with a single layer are unrealistic, we are not limited to one dimension. In fact, depth is typically more effective at creating complex representations rather than width in neural networks. Depth is measured by the count of hidden layers stacked one on top of the other in the network. Hence, the term deep neural networks. \n",
    "\n",
    "The Figure 2.4 below shows the results of an empirical study by Goodfellow, Shlens and Szegedy (2014) of accuracy of the network vs depth. Notice that accuracy increases rapidly with depth until about 8 layers, after which the effect is reduced. \n",
    "\n",
    "\n",
    "<img src=\"Figures/Accuracy-Layers.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center>Figure 2.4 Empirical results of accuracy vs. number of layers  \n",
    "Diagram from Goodfellow et. al. 2014</center>\n",
    "\n",
    "Another view of the empirical study by Goodfellow et. al. is shown in Figure 2.5 below. In this case accuracy verses number of model parameters is compared for three different network architectures. The deeper network (11 layers) makes more  efficient use of the parameters in terms of improved accuracy. The number of parameters in a layer is approximately the total number of parameters divided by the number of layers. Notice that for the particular case tested convolutional neural networks are more efficient than fully-connected networks. We will discuss convolutional neural networks in a subsequent lesson. \n",
    "\n",
    "Of particular interest is the fact that the fully-connected network and the shallow convolutional neural network appear to be over-fitting as the test accuracy actually decreases as the number of parameters increases. We will discuss the significant problems of over-fitting in neural networks in a subsequent lesson. \n",
    "\n",
    "\n",
    "<img src=\"Figures/Accuracy-Parameters.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center>**Figure 2.5 Empirical results of accuracy for different network architectures**  \n",
    "Diagram from Goodfellow et. al. 2014</center>\n",
    "\n",
    "**Summary:** Deep networks tend to produce better models, with less tendency to over-fit, for a given level of complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Computational graphs\n",
    "\n",
    "There is another way to look at neural nets, computational graphs. A computational graph breaks down the steps of a complex algorithm into steps. \n",
    "\n",
    "Computational graphs provide a way to organize complex computations in an efficient manner. Widely used computational frameworks such as Tensor Flow, CNTK, and Torch all use computational graphs. Organizing computations in a graph allows these platform to minimize memory transfers. In simple terms, the platform can look ahead in the graph and organize data and computational results so as to minimize memory transfers. As a result, such platforms can be significantly faster than, say, Python Numpy. Systems like Numpy require memory transfer before each operation, which typically take more time than the actual computation. \n",
    "\n",
    "The diagram below decomposes the single hidden layer neural network discussed in the previous section into a computational graph. \n",
    "\n",
    "\n",
    "<img src=\"Figures/CompGraph1.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Figure 2.6  \n",
    "Computational graph for fully connected neural network of Figure 2.3** </center>\n",
    "\n",
    "Let's walk though this graph, step by step.\n",
    "\n",
    "1. The $nX2$ weight tensor, $W^1$, is multiplied by the 1-dimensional input tensor $x \\in\\ R^n$, giving  the result $U^1 \\in\\ R^2$.\n",
    "2. The 1-dimensional bias tensor $b^1 \\in\\ R^2$ is added to $U^1$, giving $U^2 \\in\\ R^2$. \n",
    "2. The activation function $\\sigma_h(x)$ is applied to $U^2$, producing $U^3 \\in\\ R^2$\n",
    "3. The dot product between the weight tensor, $W^2 \\in\\ R^2$ and $U^3$ is computed giving $U^4 \\in\\ R^1$. \n",
    "4. The bias, $b^2 \\in\\ R^1$ is added to $U^4$ giving $U^5 \\in\\ R^1$.\n",
    "5. The output activation function $\\sigma_o(x)$ is applied to $U^4$ giving the output $Y \\in\\ R^1$.\n",
    "\n",
    "As you can see, the computational graph provides a complete specification for the single hidden layer neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Activation functions\n",
    "\n",
    "Without a nonlinear activation function, a neural net is just an afine transformation. Afine transformations limit representation to only linearly separable functions. To create more general representations **nonlinear activation functions** are required. \n",
    "\n",
    "In present practice, four types of activation functions are generally used for fully connected networks. \n",
    "\n",
    "1. **Linear** activation is used for the output layer of regression neural networks. \n",
    "2. The **rectilinear** activation function is used for most hidden units. The rectilinear activation function is often referred to as **ReLU**.\n",
    "3. A **leaky rectilinear** activation acts like a ReLU function for positive inputs, but has a small negative bias or leakage for negative input values. The leaky ReLU activation function can improve training for some deep neural networks. \n",
    "3. The **logistic** or **sigmoid** activation function is used for binary classifiers.\n",
    "4. The **softmax** activation function is used for multi-class classifiers. \n",
    "\n",
    "Rectilinear functions are typically used as the activation function for hidden units in neural networks. The rectilinear function is defined at:\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "The rectilinear function is linear for positive responses and zero for responses less than 0.0. Notice that the derivatives of the rectilinear function are not continuous. While this might seem to be a problem, in practice, even gradient-based optimization functions work well with this activation function. \n",
    "\n",
    "The rectilinear function is plotted in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reclu(x): \n",
    "    return(max(0,x))\n",
    "\n",
    "def plot_figs(x,y,title, figsize = (8, 6)):\n",
    "    plt.figure(figsize=figsize).gca() # define axis\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.plot(x, y)\n",
    "    plt.ylim((-0.1,1.1))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "x = np.linspace(-1.0, 1.0, 200)\n",
    "y = [reclu(y) for y in x]\n",
    "plot_figs(x,y,'The Rectilinear Function')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another widely used activation function is the **logistic** or **sigmoid**. The sigmoid is used as the activation for the output layer of a binary classifier. The general sigmoid function can be written as:\n",
    "\n",
    "$$\\sigma(x) = \\frac{L}{1 + e^{-k(x_0-x)}}\\\\\n",
    "where\\\\\n",
    "L = max\\ value\\\\\n",
    "k = slope\\\\\n",
    "x_0 = sigmoid\\ midpoint$$\n",
    "\n",
    "With $L=1$, $k=1$, and $x_0 = 0$, the logistic function becomes: \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}$$\n",
    "\n",
    "The sigmoid function can asymptotically approach $0$ or $1$, but will never reach these extreme values. However, because of the rapid decrease in the derivative away from $0$ the sigmoid can **saturate** when using gradient-based training. For this reason, the sigmoid is typically not used for hidden layers in neural networks.   \n",
    "\n",
    "When used in a the binary classifier a threshold is set to determine if the result is $0$ or $1$. The threshold can be adjusted to bias the result as desired. \n",
    "\n",
    "The code in the cell below plots the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x): \n",
    "    return exp(x)/(1 + exp(x))\n",
    "\n",
    "x = np.linspace(-8.0, 8.0, 200)\n",
    "y = [sigmoid(y) for y in x]\n",
    "plot_figs(x,y,'The Logistic Function') #, figsize = (5,3))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **softmax** function or **normalized exponential function** is used for the output activation function of a multi-class classifiers. The softmax function is the multinomial generalization of the sigmoid or logistic function. The probability of each class $j$ is written as: \n",
    "\n",
    "$$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n",
    "\n",
    "The normalization $\\sum_{k=1}^K e^{z_k}$ ensures the sum of probabilities for all classes add to $1.0$. The class selected by the classifier is the class with the largest value of $\\sigma(z_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Computational example\n",
    "\n",
    "Now that we have gone though some basic theory for feed-forward networks, it's time try a simple example. You will construct a fully connected network to compute this simple function:\n",
    "\n",
    "$$y = x_1 - x_2$$\n",
    "\n",
    "****\n",
    "**Comment.** You have likely have noticed that this function is linear and can be computed easily without a neural network. Of course, that is not the point. We use a simple function to make the results easy to understand. \n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-1:** You will create and test a simple neural network implemented using matrix multiplication with Numpy. The architecture of the neural network is similar to the one shown in Figure 2.3, with 2 input units, 2 hidden units and 1 output unit. The neural network for this example does not require any bias terms. \n",
    "> 1. As a first step, create test data for 3 cases; $x_1 > x_2$, $x_1 = x_2$, and $x_1 <x_2$, and with positive and negative values, or $x = [(2,1), (1,1), (1,2), (-2,1), (-1,-1), (-1,2)]$ as the input tuples. \n",
    "> 2. Directly compute and print the evaluation of the function, $y = x_1 - x_2$, for each tuple.  \n",
    "\n",
    "****\n",
    "**Note:** The network you are asked to construct is simple and all weights must be in the set $\\{-1, 1 \\}$. You can take advantage of the symmetry of the function you must approximate to determine these weights by inspection. If you wish, you will find it easy to compute the partial derivatives of the function to be approximated. However, this is not necessary if you carefully inspect the network and consider the responses required. \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "-1\n",
      "-3\n",
      "0\n",
      "-3\n"
     ]
    }
   ],
   "source": [
    "## Your code goes here\n",
    "x = [(2,1),\n",
    "     (1,1),\n",
    "     (1,2),\n",
    "     (-2,1),\n",
    "     (-1,-1),\n",
    "     (-1,2)]\n",
    "\n",
    "X1 = [2,1,1,-2,-1,-1]\n",
    "X2 = [1,1,2,1,-1,2]\n",
    "\n",
    "X_array = np.array([X1,X2]).T\n",
    "\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    difference = x[i][0] - x[i][1]\n",
    "    y.append(difference)\n",
    "    print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Now that you have our test data and test cases you can move to the next step. Determine the values of the $2 \\times 2$ weight tensor between the input layer and the hidden layer and the $2 \\times 1$ weight tensor between the hidden and output layer. In the code cells below create as Numpy arrays and print these tensors.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m x n = row x columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "w1 = (-1,1)\n",
    "w2 = (1,-1)\n",
    "\n",
    "input_hidden_w = [w1,\n",
    "                 w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, 1), (1, -1)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_hidden_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "wh1 = -1\n",
    "wh2 = 1\n",
    "hidden_output_w = [wh1,\n",
    "                  wh2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_output_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Now, it is time to compute the results and check them. To create the computational process follow the graph in Figure 2.6, but ignoring the bias terms, $b^1$ and $b^2$. \n",
    "> Create a function `hidden` to compute the output of the hidden layer using the formulation with rectalinear activation:   \n",
    "> $$h = \\sigma(W^1 \\cdot x)$$\n",
    "> Create a second function to computes the vector product of the weight vector with the output vector of the hidden layer using the following formulation with linear activation:    \n",
    "> $$o = W^2 \\cdot h$$\n",
    "> 5. Execute the two functions while iterating over the input tuples and verify the output is correct. If not, reconsider the values of your weight tensors.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "def hidden(W1, X, bias = 0):\n",
    "    Z1 = np.dot(W1,X) + bias\n",
    "    A1 = []\n",
    "    for i in range(len(Z1)):\n",
    "        neuron = reclu(Z1[i])\n",
    "        A1.append(neuron)\n",
    "    return np.array(A1)\n",
    "\n",
    "def output(W2, A1):\n",
    "    Z2 = np.dot(W2,A1)\n",
    "    return Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the test cases and check the results     \n",
    "# input parameters\n",
    "hidden_values = []\n",
    "for i in range(len(x)):\n",
    "    hidden_tensor = hidden(input_hidden_w, x[i])\n",
    "    hidden_values.append(hidden_tensor)\n",
    "    \n",
    "outputs = []\n",
    "for i in range(len(hidden_values)):\n",
    "    output_product = output(hidden_output_w, hidden_values[i])\n",
    "    outputs.append(output_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, -1, -3, 0, -3]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, -1, -3, 0, -3]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Original difference values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Congratulations! Your first fully connected neural network passed all the tests!    \n",
    "> **End of exercise**.\n",
    "\n",
    "Notice that even a network to compute a simple function requires 6 weights. You can see that for more complex functions any practical algorithm must learn a large number of weights. The limitations of Numpy would quickly become evident for large scale problems involving hundreds of millions of weights. \n",
    "\n",
    "****\n",
    "**Note:** If you are having difficulty following the Numpy code in the above example, you might want to look at [Scott Shell's Numpy Tutorial](https://engineering.ucsb.edu/~shell/che210d/numpy.pdf)\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-2:** You will now construct and test an neural network implementing an [exclusive or function, the XOR](https://en.wikipedia.org/wiki/Exclusive_or). The XOR function outputs a 1 if either input is 1 and the other 0, and a 0 otherwise. You can use the `hidden` and `output` functions you created for the previous exercise. Make sure you try all 4 possible test cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "inputs = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "#Weights from input to hidden layer\n",
    "w1_XOR = (1,1)\n",
    "w2_XOR = (1,1)\n",
    "\n",
    "input_hidden_w_XOR = [w1_XOR,\n",
    "                 w2_XOR]\n",
    "\n",
    "#Weights from hidden layer to output\n",
    "wh1_XOR = 1\n",
    "wh2_XOR = -2\n",
    "hidden_output_w_XOR = [wh1_XOR,\n",
    "                  wh2_XOR]\n",
    "\n",
    "#Bias term\n",
    "b1 = 0\n",
    "b2 = -1\n",
    "bias_hidden_w = [b1,\n",
    "                b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code goes here\n",
    "XOR_hidden = []\n",
    "for i in range(len(inputs)):\n",
    "    XOR_hidden_tensor = hidden(input_hidden_w_XOR,inputs[i], bias_hidden_w)\n",
    "    XOR_hidden_tensor = XOR_hidden_tensor\n",
    "    XOR_hidden.append(XOR_hidden_tensor)\n",
    "    \n",
    "XOR_outputs = []\n",
    "for i in range(len(XOR_hidden)):\n",
    "    output_product_XOR = output(hidden_output_w_XOR, XOR_hidden[i])\n",
    "    XOR_outputs.append(output_product_XOR)\n",
    "    \n",
    "XOR_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output parameters\n",
    "outputs = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Learning in neural networks: Backpropagation\n",
    "\n",
    "Now that we have a promising representation, we need to determine if it is trainable. The answer is not only yes we can, but that we can do so in a computationally efficient manner, using a cleaver algorithm known as **backpropagation**. \n",
    "\n",
    "The backpropagation algorithm was developed independently multiple times. The earliest work on this algorithm was by Kelly (1960) in the context of control theory and Bryson (1961) in the context of dynamic programming. Rumelhart, Hinton and Williams (1984) demonstrated empirically that backpropagation can be used to train neural networks. Their paper marks the modern history of neural networks, and set off the first wave of enthusiasm. \n",
    "\n",
    "The backpropagation algorithm requires several components. First, we need a **loss function** to measure how well our representation matches the function we are trying to learn. Second, we need a way to propagate changes in the representation through the complex network For this we will use the **chain rule of calculus** to compute **gradients** of the representation. In the general case, this process requires using automatic differentiation methods. \n",
    "\n",
    "The point of backpropagration is to learn the optimal weight for the neural network. The algorithm proceeds iteratively through a series of small steps. Once we have the gradient of the loss function we can update the tensor of weights.\n",
    "\n",
    "$$W_{t+1} = W_t + \\alpha \\nabla_{W} J(W_t) $$  \n",
    "where  \n",
    "$W_t = $ the tensor of weights or model parameters at step $t$.   \n",
    "$\\alpha\\ = $ step size or learning rate.  \n",
    "$J(W) = $ loss function given the weights.  \n",
    "$\\nabla_{W} J(W) = $ gradient of $J$ with respect to the weights $W$.  \n",
    "\n",
    "It should be evident that the back propagation algorithm is a form of gradient decent. The weights are updated in small steps following the gradient of $J(W)$ down hill. \n",
    "\n",
    "Finally, we need a way evaluate the performance of the model. Without evaluation metrics we have no way to compare the performance of a given model, or compare the performance of several models. \n",
    "\n",
    "In the next sections, we will address each of loss functions, gradient computation and performance measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loss functions\n",
    "\n",
    "To train a neural network we must have a **loss function**, also known as a **cost function**. In simple terms, the loss function measures the fit of a model to the training data. The lower the loss, the better the fit. \n",
    "\n",
    "To train deep learning models **cross entropy** is often used as a loss function. This is an information theoretic measure of model fit. We can understand cross entropy as follows. \n",
    "\n",
    "First define **Shannon entropy** as:\n",
    "\n",
    "$$\\mathbb{H}(I) = E[I(X)] = E[-ln_b(P(X))] = - \\sum_{i=1}^n P(x_i) ln_b(P(x_i)$$  \n",
    "Where:  \n",
    "$E[X] = $ the expectation of $X$.  \n",
    "$I(X) = $ the information content of $X$.   \n",
    "$P(X) = $ probability of $X$.  \n",
    "$b = $ base of the logarithm.    \n",
    "\n",
    "This rather abstract formula gives us a way to compute the expected information content of a set of values $X$. The more likely (higher probability) of $X$ the less informative it is. \n",
    "\n",
    "To create a loss function from the definition of Shannon entropy we start with the **Kullback-Leibler divergence (KL divergence)** or **relative entropy**. The KL divergence is an information theoretic measure of the difference between two distributions, $P(X)$ and $Q(X)$.\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b \\frac{p(x_i)}{q(x_i)}$$\n",
    "\n",
    "Ideally, in the case of training a machine learning model we want a distribution $Q(X)$, which is identical to the actual data distribution $P(X)$. \n",
    "\n",
    "But, you may say, if we could know $P(X)$ why compute $Q(X)$ at all? Fortunately, we do not have to. We can rewrite the KL divergence as:\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = \\sum_{i=1}^n p(x_i)\\ ln_b p(x_i) - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n",
    "\n",
    "Since $P(X)$ is fixed and we wish to find $Q(X)$ when we train our model, we can minimize the term on the right, which is the **cross entropy** defined as:\n",
    "\n",
    "$$\\mathbb{H}(P,Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n",
    "\n",
    "From the formulation of KL divergence above you can see the following.\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = \\mathbb{H}(P) + \\mathbb{H}(P,Q)\\\\\n",
    "\\mathbb{D}_{KL}(P \\parallel Q) = Entropy(P) + Cross\\ Entropy(P,Q)$$\n",
    "\n",
    "Thus, we can minimize divergence by minimizing cross entropy. This idea is both intuitive and computationally attractive. The closer the estimated distribution $q(X)$ is to the distribution of the true underling process $p(X)$, the lower the cross entropy and the lower the KL divergence. \n",
    "\n",
    "In general we will not know $p(X)$. In fact, if we did, why would we need to solve a training problem? So, we can use the following approximation.\n",
    "\n",
    "$$\\mathbb{H}(P,Q) = - \\frac{1}{N} \\sum_{i=1}^n ln_b q(x_i)$$\n",
    "\n",
    "You may notice, that this approximation, using the average log likelihood, is equivalent to a maximum likelihood estimator (MLE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a specific case of a model with Gaussian likelihood. What is the cross entropy? We can start by thinking about the definition of likelihood. \n",
    "\n",
    "$$p(data|model) = p(data|f(\\theta)) = p(x_i|f(\\hat{\\mu},\\sigma))= \\frac{1}{2 \\pi \\sigma^2} e^{\\frac{-(x_i - \\hat{\\mu})^2}{2 \\sigma^2}}$$\n",
    "\n",
    "We take the negative logarithm of this likelihood model. \n",
    "\n",
    "$$-log\\big(p(data|model) \\big) = - \\frac{1}{2}\\big( log( 2 \\pi \\sigma^2) + \\frac{(x_i - \\hat{\\mu})^2}{2 \\sigma^2} \\big)$$\n",
    "\n",
    "Now, the first term on the right is a constant, as is the denominator of the second term if we assume known variance. Since our goal is to minimize cross entropy, we can eliminate these quantities and be left with just the following.\n",
    "\n",
    "$$-(x_i - \\hat{\\mu})^2$$\n",
    "\n",
    "This is one issue we need to deal with. Our formulation of cross entropy involves the unknown true distribution of the underling process $p(X)$. However, since $p(x_i)$ is fixed but unknown we can just write the following.\n",
    "\n",
    "$$min \\big( \\mathbb{H}(P,Q) \\big) \\propto argmin_{\\mu} \\big( - \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 \\big)$$\n",
    "\n",
    "This is just the definition of a Maximum Likelihood Estimator (MLE) for the least squares problem! In fact, since the cross entropy is computed using the negative log likelihood, it will always be minimized by the MLE. \n",
    "\n",
    "You can see another example of [cross-entropy error function and logistic regression](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computing Loss functions\n",
    "\n",
    "The loss function is used to train the model. Therefore the loss function must be computed in an efficient manner. \n",
    "\n",
    "Given the number of parameters in deep neural nets over-fitting is inevitable. Therefore some regularization is required. We will discuss regularization in greater depth in another lesson. For now, we will just use the following regularized form.\n",
    "\n",
    "$$\\mathbb{H}(P,Q) = J(\\theta) = - \\frac{1}{N}\\sum_{i=1}^n ln_b q(x_i|\\theta) + \\lambda ||\\theta||^2\\\\ \n",
    "where\\\\\n",
    "- \\frac{1}{N}\\sum_{i=1}^n ln_b q(x_i|\\theta) = J_{MLE}(\\theta)\\\\\n",
    "||\\theta||^2 = L^2\\ norm\\ regularization\\ term$$\n",
    "\n",
    "To minimize $J(\\theta)$ in this form $\\theta$ must be chosen to keep $||\\theta||^2$ small while minimizing the negative log likelihood of $q(x_i|\\theta)$.\n",
    "\n",
    "Let's  consider how we would compute this form of the lost function. The computational graph shown below illustrates the computational path for the regularize d loss function. For simplicity, no bias terms are considered.\n",
    "\n",
    "<img src=\"Figures/CompGraph2.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>Figure 3.1  \n",
    "Computational graph for computing loss of fully connected neural network of Figure 2.3 </center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Chain rule of calculus\n",
    "\n",
    "Key to the back propagation algorithm is the chain rule of calculus; not to be confused with the chain rule of probability. The chain rule allows us to back propagate gradients though an arbitrarily complex graph of functions. \n",
    "\n",
    "Now, suppose there is a function $y = g(x)$, and another function $z = f(y) = f(g(x))$. How do we compute the derivative of $z$ with respect to $x$? Applying the chain rule we get: \n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx}$$\n",
    "\n",
    "Consider $x \\in R^M$ $g(x) \\Rightarrow R^M$ and $ f(y) \\Rightarrow z \\in R$. The chain rule becomes:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\sum_{j \\in M} \\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$$\n",
    "\n",
    "Which we can rewrite as  \n",
    "\n",
    "$$\\nabla_{x}z = \\Big( \\frac{\\partial x}{\\partial y} \\Big)^T \\nabla_{y}z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\frac{\\partial x}{\\partial y}$ is the $n x m$ **Jacobian matrix** of partial derivatives. The Jacobian is multiplied by the gradient with respect to $y$, $\\nabla_{y}z$. You can think of the Jacobian as a transformation for a gradient with respect to $y$ to what we really want, the gradient with respect to $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Example of finding a gradient. \n",
    "\n",
    "Let's work out backpropagation for a very simple neural network with a just an input layer and an output layer. This neural network, including the loss function, is shown in Figure 3.2 below. This network has been highly simplified. There are only three layers, input layer, a two unit hidden layer with no bias terms, and a single unit output layer. There are only two weight tensors for this network. Further, the hidden units use rectilinear activation and the output unit uses linear activation. These activation functions have simple partial derivatives.  \n",
    "\n",
    "<img src=\"Figures/LossGraph.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>Figure 3.2 \n",
    "Simple single layer neural network with loss function </center>\n",
    "\n",
    "To analyze this network we will refer to the computational graph shown in Figure 3.1 above. \n",
    "\n",
    "First, we need to work out the forward propagation relationships. We can compute the outputs of the hidden layer as follows.\n",
    "\n",
    "$$S_{\\{1,2\\}} = \\sigma_h \\big( W^1 \\cdot X_{\\{1,2\\}} \\big) = \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$  \n",
    "\n",
    "In the same way, the result from the output layer can be computed as follows, since the activation function for this layer is linear. \n",
    "\n",
    "$$S_3 = W^2 \\cdot S_{\\{1,2\\}} = \\sum_i W^2_i \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform backpropagation, we need fill out the gradient vector by computing $\\frac{\\partial J(W)}{\\partial W}$ for each weight in the model. \n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J(W)}{\\partial W^2_{11}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^2_{12}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^2_{21}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^2_{22}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^1_{1}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^1_{2}}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple in this example we will just use a non-normalized squared error loss function. This is just the MLE estimator (without normalization) for a Gaussian distribution. \n",
    "\n",
    "$$J(W) = - \\frac{1}{2} \\sum_{l=1}^n (y_l - S_{3,l})^2 $$\n",
    "\n",
    "Where:  \n",
    "$y_k = $ the label for the lth case.     \n",
    "$\\hat{y_k} = S_{3,k} =$ the output of the network for the lth case. \n",
    "\n",
    "We want to compute the gradients with respect to the input and output tensors:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^1}, \\ \\frac{\\partial J(W)}{\\partial W^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the easier case of the partial derivatives with respect to the output tensor. We can apply the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^2_k} = \\frac{\\partial  J(W)}{\\partial S_{3,k}} \\frac{\\partial S_{3,k}}{\\partial W^2_k}$$\n",
    "\n",
    "The first partial derivative of the chain is:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial S_{3,k}} = \\frac{\\partial - \\frac{1}{2} (y_k - S_{3,k})^2} {\\partial S_{3,k}} = y_k - S_{3,k} $$\n",
    "\n",
    "And, the partial derivative of the second partial derivative in the chain, given the linear activation of the output unit:\n",
    "\n",
    "$$\\frac{\\partial S_{3,k}}{\\partial W^2_k} = \\frac{\\partial W^2_k S_{j,k}}{\\partial W^2_k}  = S_{j,l}, \\ j \\in \\{1,2\\}$$\n",
    "\n",
    "Multiplying the two components of the chain gives us:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^2_k} = S_{j,k} (y_k - S_{3,k}), \\ j \\in \\{1,2\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives with respect to the input tensor are a bit more complicated. To apply the chain rule we must work backwards from the loss function. This gives the following chain:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^1_{i,j}} =  \\frac{\\partial J(W)}{\\partial S_{3}} \\frac{\\partial S_{3}}{\\partial S_{j}} \\frac{\\partial S_{j}}{\\partial W^1_{i,j}}$$\n",
    "\n",
    "First, we find the right most partial derivative in our chain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S_j}{\\partial W^1_{i,j}} = \n",
    "\\begin{cases}\n",
    "     \\frac {\\partial W^1_{i,j} x_{i,k}}{\\partial W^1_{i,j}}, & \\text{if $S_j>0$} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Which given the ReLU activation results in:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S_j}{\\partial W^1_{i,j}} = \n",
    "\\begin{cases}\n",
    "    1, & \\text{if $S_j>0$}  \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The middle partial derivative must account for the nonlinearity:\n",
    "\n",
    "$$\\frac{\\partial S_{3}}{\\partial S_{j}} = W^2_j$$\n",
    "\n",
    "We have already computed $\\frac{\\partial J(W)}{\\partial S_{3}}$. Multiplying all three partial derivatives we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(W)}{\\partial W^1_{i,j}} = \n",
    "\\begin{cases}\n",
    "    (y_k - S_{3,k}) W^2_j, & \\text{if $S_j>0$} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Where $S_3$ and $S_{\\{1,2 \\}}$ are computed using the relationships given above. \n",
    "\n",
    "A more detailed, but still digestable example of computing gradients for backpropagation can be found in a blog post by [Manfred Zaharauskas](http://blog.manfredas.com/backpropagation-tutorial/), among many other places. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Creating a Model With Keras      \n",
    "\n",
    "You will now create and test a first deep learning classifier model for the MNIST dataset using Keras. The fully connected model has one hidden layer.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preparing the Dataset      \n",
    "\n",
    "You have already worked with the MNIST dataset. To load these data and prepare them for the Keras model execute the code in the cell below.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28*28)).astype('float32')/255\n",
    "print(train_images.shape)\n",
    "test_images = test_images.reshape((10000, 28*28)).astype('float32')/255\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more preprocessing step. The labels need to be [**one hot encoded**](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding). One hot encoding transforms an $N$ level categorical variable into $N$ binary columns. One column represents one category. A 1 or binary true value is encoded in the column of a given category with the other columns coded as 0 or false.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-3:** You will now one hot encoded the label vectors of the training and test data. Use the [keras.utils.np_utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function to create the one hot encoded labels. Print the first 10 rows of the training labels. You will need to set options to display all columns.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the printed one hot encoded labels. Does the number of columns correspond to the number of label categories?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Defining and Executing the Deep Learning Model\n",
    "\n",
    "It is now time to define the Keras neural network model. This model uses the [Keras **Sequential** class](https://keras.io/guides/sequential_model/). The model is constructed by **adding dense layers** with the `add` method. Hidden and output layers are specified by creating instances of the [Dense layer class](https://keras.io/api/layers/core_layers/dense/). An input data shape must be specified only for the input layer.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-4:** You will now specify, [compile and fit](https://keras.io/api/models/model_training_apis/) the neural network model by the following steps:     \n",
    "> 1. Define the sequential model by instantiating a model object using `models.Sequential`. Name your model `nn`. Then add layers:  \n",
    ">   - Add a dense input layer with $28 \\times 28$ hidden units, rectalinear (`relu`) activation and `input_shape=(28*28, )`.   \n",
    ">   - Add a dense hidden layer with 512 hidden units and rectalinear (`relu`) activation.     \n",
    ">   - Add a dense output output layer with `activation='softmax'`.     \n",
    "> 2. Compile your model with the following arguments; `optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']`. We will discuss optimizers in another lesson. \n",
    "> 3. Print a summary of the model with the `summary()` method. \n",
    "> 3. Fit your model using the training images, training labels, and arguments; `epochs=5, batch_size=128`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer these questions:  \n",
    "> 1. Why is the `categorical_crossentropy` the good choice loss function and `softmax` the correct choice for output activation for this model? \n",
    "> 2. Considering the number of training samples, and number of trainable model parameters, where do you think this model might lie along the bias-variance trade-off spectrum?   \n",
    "> 2. Examine the evolution of the loss function and accuracy. What do these figures tell you about learning for this model?  \n",
    "> **End or exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1. \n",
    "> 2. \n",
    "> 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Metrics\n",
    "\n",
    "Now that we have the components for training a basic neural network in place we need a way to evaluate the performance. It turns out, there is nothing special evaluation of neural network models as opposed to other machine learning models. For regression models, one typically use the standard metrics such as root mean square error (RMSE), mean absolute error (MAE). For classification models, one also typically uses the standard metrics including the confusion matrix, accuracy, precision and recall. The Keras [metrics package](https://keras.io/api/metrics/) provides numerous methods for model evaluation. \n",
    "\n",
    "Execute the code in the cell below to compute and display performance metrics for your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-5:** Compare the results of the evaluation with the same metrics achieved during model training. What does the difference tell you about the generalization for this model?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Using Training History   \n",
    "\n",
    "The Keras model `fit` method creates a [TensorFlow history object using callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History). The information contained in the history object can be very useful in understanding model training; what is working and what is not.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-6:** You will now re-train your model while capturing a history. To retrain a model you must re-compile it first to create a fresh model object. Training an existing model object will continue the training of that object. This property of Keras models can be most useful for updating existing models. Do the following:      \n",
    "> 1. Compile the existing model using the same arguments.     \n",
    "> 2. Fit the model as before, but for 10 epochs and with an additional argument; `validation_data=(test_images, test_labels)`. Assign the results to a name `history_nn`.   \n",
    "> 3. Execute the code in the next two cells to display charts of training and test loss and accuracy.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red', label = 'test loss')\n",
    "    plt.plot(x, train_loss, label = 'traning loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(history_nn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    train_acc = history.history['accuracy']\n",
    "    test_acc = history.history['val_accuracy']\n",
    "    x = list(range(1, len(test_acc) + 1))\n",
    "    plt.plot(x, test_acc, color = 'red', label = 'test accuracy')\n",
    "    plt.plot(x, train_acc, label = 'training accuracy')  \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch')  \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy(history_nn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the training and test loss and accuracy. Notice the trajectory of the test accuracy vs. the training accuracy. What does the lack of convergence of these losses tell you about learning of |the generalization of the model?   \n",
    "> **End of exercise.**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Adding Regularization        \n",
    "\n",
    "You have seen some of the effects of over-fitting of the neural network model. **Regularization** methods are widely used in machine learning to prevent over-fitting. Conceptually, you can think of regularization as moving the model toward lower variance and higher bias to improve generalization. We will examine regularization in greater detail in another lesson.      \n",
    "\n",
    "Keras used the [layer weight regularizer class](https://keras.io/api/layers/regularizers/) to add weight constraints to the to layers. Here, we will only used the L2 regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1-7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-7:** You will now add L2 regularization to the input and hidden layers of your model. Do the following:    \n",
    "> 1. Starting with the model specification you have been using, add the following argument to the input and hidden layer; `kernel_regularizer=regularizers.l2(0.1)`.    \n",
    "> 2. Compile the model.   \n",
    "> 3. Fit the model for 20 epochs, saving the history object.   \n",
    "> 4. Plot the training and test loss and accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the charts of loss and accuracy for the regularized model with those from training the unregularized model and answer these questions:    \n",
    "> 1. Do the charts and numeric metrics for the regularized model show an improvement in the generalization of the model compared to the unregularized model, and why?      \n",
    "> 2. Can you see evidence in the charts and numeric metrics that the regularized model has greater bias compared to the unregularized model? \n",
    "> 2. Notice that the test and training loss of the regularized model continue to improve. Do you think that training for additional epochs will be beneficial? \n",
    "> **End of exercise.**      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.     \n",
    "> 2. \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018, 2019, 2020, 2021, 2022, Stephen F Elston. All rights reserved"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
